{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e417a811-f03b-416f-acae-8212f0b4030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5296ac32-aec0-4a55-8044-f648c8929f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed25a25-9be7-48cd-8f96-2ad4e13dc452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preparation\n",
    "class CoconutTreeDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        boxes = self.annotations.iloc[idx, 1:5].values.astype(float)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        label = torch.tensor([1])  # 1 for coconut_tree\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, boxes, label\n",
    "\n",
    "# Define transforms with data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54f806c9-bdd9-4c14-b601-38afbe5ac582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "full_dataset = CoconutTreeDataset(csv_file='../data/annotation_data.csv', img_dir='../data/raw_data', transform=transform)\n",
    "\n",
    "# Split the data\n",
    "train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Subset objects\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset = Subset(full_dataset, val_idx)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fefeff6-cfe8-4d29-9cda-fcd2597f3bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Architecture\n",
    "class CoconutTreeDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CoconutTreeDetector, self).__init__()\n",
    "        self.base_model = models.resnet50(pretrained=True)\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Linear(self.base_model.fc.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4)  # 4 for bounding box coordinates\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e55dd342-b45a-4dd9-9c1e-ee5545513fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "model = CoconutTreeDetector().to(device)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2114b-8cb5-4017-9e37-d8382237db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training Loop\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, boxes, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        boxes = boxes.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, boxes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, boxes, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            boxes = boxes.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, boxes)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), '../model/model-3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a3fc4-3a87-4b16-ab2b-2037fef41b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define your updated model class\n",
    "class CoconutTreeDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CoconutTreeDetector, self).__init__()\n",
    "        self.base_model = models.resnet50(pretrained=True)\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Linear(self.base_model.fc.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4)  # 4 for bounding box coordinates\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Create an instance of your model\n",
    "model = CoconutTreeDetector().to(device)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "state_dict = torch.load('../model/model-3.pth')\n",
    "\n",
    "# Load the state dict into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define transforms for test image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Directory containing test images\n",
    "test_dir = '../data/coconut_tree_coco/valid/'\n",
    "\n",
    "# List all files in the directory\n",
    "image_files = os.listdir(test_dir)\n",
    "\n",
    "# Loop through each image file\n",
    "for image_file in image_files:\n",
    "    # Load and preprocess the test image\n",
    "    image_path = os.path.join(test_dir, image_file)\n",
    "    test_image = Image.open(image_path).convert(\"RGB\")\n",
    "    test_image_tensor = transform(test_image)\n",
    "    test_image_tensor = test_image_tensor.unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_image_tensor)\n",
    "\n",
    "    # Assuming outputs contain bounding box predictions, you can interpret them\n",
    "    predicted_boxes = outputs.squeeze().cpu().numpy()\n",
    "\n",
    "    # Visualize the prediction on the image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(test_image)\n",
    "    current_axis = plt.gca()\n",
    "\n",
    "    # Draw predicted bounding box\n",
    "    xmin, ymin, xmax, ymax = predicted_boxes\n",
    "    rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, edgecolor='red', linewidth=2)\n",
    "    current_axis.add_patch(rect)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Prediction for {image_file}')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
