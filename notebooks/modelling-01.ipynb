{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/agkav/miniconda3/envs/PyTorch-Development/python\n",
      "Fri Jul  5 16:20:05 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.99                 Driver Version: 555.99         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   40C    P8              1W /  125W |       0MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    165732    C+G   ...8bbwe\\SnippingTool\\SnippingTool.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\agkav\\miniconda3\\envs\\pytorch-development\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\agkav\\miniconda3\\envs\\pytorch-development\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\agkav\\miniconda3\\envs\\pytorch-development\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\agkav\\miniconda3\\envs\\pytorch-development\\lib\\site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\agkav\\miniconda3\\envs\\pytorch-development\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\agkav\\miniconda3\\envs\\pytorch-development\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\agkav\\miniconda3\\envs\\pytorch-development\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\agkav\\miniconda3\\envs\\pytorch-development\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\agkav\\miniconda3\\envs\\pytorch-development\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\agkav\\miniconda3\\envs\\pytorch-development\\lib\\site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\agkav\\miniconda3\\envs\\pytorch-development\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "\n",
    "class CoconutTreeDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        boxes = self.annotations.iloc[idx, 1].split()\n",
    "        boxes = [float(coord) for coord in boxes]\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "        \n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)  # 1 for coconut_tree\n",
    "        \n",
    "        if self.transform:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "        \n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "class ComposeTransforms:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, boxes):\n",
    "        for t in self.transforms:\n",
    "            img, boxes = t(img, boxes)\n",
    "        return img, boxes\n",
    "\n",
    "class Resize:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img, boxes):\n",
    "        orig_size = torch.tensor([img.width, img.height, img.width, img.height])\n",
    "        img = F.resize(img, self.size)\n",
    "        scale_factor = torch.tensor([self.size[1], self.size[0], self.size[1], self.size[0]]) / orig_size\n",
    "        boxes = boxes * scale_factor\n",
    "        return img, boxes\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, img, boxes):\n",
    "        if torch.rand(1) < self.prob:\n",
    "            img = F.hflip(img)\n",
    "            boxes[:, [0, 2]] = img.width - boxes[:, [2, 0]]\n",
    "        return img, boxes\n",
    "\n",
    "class RandomVerticalFlip:\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, img, boxes):\n",
    "        if torch.rand(1) < self.prob:\n",
    "            img = F.vflip(img)\n",
    "            boxes[:, [1, 3]] = img.height - boxes[:, [3, 1]]\n",
    "        return img, boxes\n",
    "\n",
    "class RandomRotation:\n",
    "    def __init__(self, degrees):\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, img, boxes):\n",
    "        angle = torch.empty(1).uniform_(-self.degrees, self.degrees).item()\n",
    "        img = F.rotate(img, angle)\n",
    "        # Bounding box rotation can be complex and might require more specific handling.\n",
    "        return img, boxes\n",
    "\n",
    "# Define transforms with data augmentation\n",
    "transform = ComposeTransforms([\n",
    "    Resize((224, 224)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "    RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CoconutTreeDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        boxes = self.annotations.iloc[idx, 1].split()\n",
    "        boxes = [float(coord) for coord in boxes]\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "        \n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)  # 1 for coconut_tree\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return image, target\n",
    "\n",
    "# Define transforms with data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "full_dataset = CoconutTreeDataset(csv_file=r'D:\\code\\CoconutCount\\data\\train\\mbb_noempties.csv', img_dir= r'D:\\code\\CoconutCount\\data\\train\\jpegs', transform=transform)\n",
    "\n",
    "# Split the data\n",
    "train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Subset objects\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset = Subset(full_dataset, val_idx)\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return images, targets\n",
    "\n",
    "# Create dataloaders with the custom collate function\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Check a sample batch from train_loader\n",
    "images_batch, targets_batch = next(iter(train_loader))\n",
    "print(images_batch[0].shape, targets_batch[0]['boxes'].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv_file=r'D:\\code\\CoconutCount\\data\\train\\mbb_noempties.csv', img_dir= r'D:\\code\\CoconutCount\\data\\train\\jpegs'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
