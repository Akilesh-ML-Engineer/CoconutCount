{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb973ded-f1f5-4d67-818b-d9a7722d73cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asa/anaconda3/envs/PyTorch-Development/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e871e3ce-d89e-40c4-a323-b3401376ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f31d481-e25d-43aa-89c4-c7b13111edbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c05dbba6-8985-43c9-9b79-6494e9600fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preparation\n",
    "class CoconutTreeDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        boxes = self.annotations.iloc[idx, 1:5].values.astype(float)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32).view(-1, 4)\n",
    "        \n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)  # 1 for coconut_tree\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "# Define transforms with data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1db7d3-9fcf-4c78-8211-aad782066035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "full_dataset = CoconutTreeDataset(csv_file='../data/annotation_data.csv', img_dir='../data/raw_data', transform=transform)\n",
    "\n",
    "# Split the data\n",
    "train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Subset objects\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset = Subset(full_dataset, val_idx)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb6eff2-00d7-4513-b4b6-48a9a0a7f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    # Load an instance of Faster R-CNN with the best available pre-trained weights\n",
    "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "    model = fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "model = get_model(num_classes=2)  # 1 class (coconut_tree) + background\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "lr_scheduler = StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96ca19-fdbf-4349-97e8-f36adeb8ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/15\n",
      "Batch 1/1946, Loss: 0.9992, Time: 0.83s\n",
      "Batch 2/1946, Loss: 0.6548, Time: 0.82s\n",
      "Batch 3/1946, Loss: 0.3495, Time: 0.82s\n",
      "Batch 4/1946, Loss: 0.2967, Time: 0.82s\n",
      "Batch 5/1946, Loss: 0.2253, Time: 0.82s\n",
      "Batch 6/1946, Loss: 0.2036, Time: 0.82s\n",
      "Batch 7/1946, Loss: 0.2837, Time: 0.82s\n",
      "Batch 8/1946, Loss: 0.2341, Time: 0.82s\n",
      "Batch 9/1946, Loss: 0.3198, Time: 0.82s\n",
      "Batch 10/1946, Loss: 0.3922, Time: 0.82s\n",
      "Batch 11/1946, Loss: 0.4103, Time: 0.82s\n",
      "Batch 12/1946, Loss: 0.3812, Time: 0.82s\n",
      "Batch 13/1946, Loss: 0.4266, Time: 0.82s\n",
      "Batch 14/1946, Loss: 0.3114, Time: 0.82s\n",
      "Batch 15/1946, Loss: 0.2878, Time: 0.83s\n",
      "Batch 16/1946, Loss: 0.3123, Time: 0.82s\n",
      "Batch 17/1946, Loss: 0.2586, Time: 0.82s\n",
      "Batch 18/1946, Loss: 0.3924, Time: 0.82s\n",
      "Batch 19/1946, Loss: 0.2712, Time: 0.82s\n",
      "Batch 20/1946, Loss: 0.3905, Time: 0.82s\n",
      "Batch 21/1946, Loss: 0.3304, Time: 0.82s\n",
      "Batch 22/1946, Loss: 0.3402, Time: 0.82s\n",
      "Batch 23/1946, Loss: 0.2999, Time: 0.82s\n",
      "Batch 24/1946, Loss: 0.3055, Time: 0.82s\n",
      "Batch 25/1946, Loss: 0.2710, Time: 0.83s\n",
      "Batch 26/1946, Loss: 0.4093, Time: 0.82s\n",
      "Batch 27/1946, Loss: 0.2854, Time: 0.82s\n",
      "Batch 28/1946, Loss: 0.2497, Time: 0.82s\n",
      "Batch 29/1946, Loss: 0.2743, Time: 0.82s\n",
      "Batch 30/1946, Loss: 0.2425, Time: 0.83s\n",
      "Batch 31/1946, Loss: 0.2279, Time: 0.82s\n",
      "Batch 32/1946, Loss: 0.2660, Time: 0.82s\n",
      "Batch 33/1946, Loss: 0.2716, Time: 0.83s\n",
      "Batch 34/1946, Loss: 0.3098, Time: 0.82s\n",
      "Batch 35/1946, Loss: 0.2594, Time: 0.83s\n",
      "Batch 36/1946, Loss: 0.2608, Time: 0.82s\n",
      "Batch 37/1946, Loss: 0.2899, Time: 0.83s\n",
      "Batch 38/1946, Loss: 0.2690, Time: 0.82s\n",
      "Batch 39/1946, Loss: 0.2389, Time: 0.83s\n",
      "Batch 40/1946, Loss: 0.2967, Time: 0.83s\n",
      "Batch 41/1946, Loss: 0.2547, Time: 0.82s\n",
      "Batch 42/1946, Loss: 0.2834, Time: 0.82s\n",
      "Batch 43/1946, Loss: 0.2501, Time: 0.83s\n",
      "Batch 44/1946, Loss: 0.2511, Time: 0.83s\n",
      "Batch 45/1946, Loss: 0.2716, Time: 0.83s\n",
      "Batch 46/1946, Loss: 0.2497, Time: 0.82s\n",
      "Batch 47/1946, Loss: 0.2976, Time: 0.83s\n",
      "Batch 48/1946, Loss: 0.2432, Time: 0.82s\n",
      "Batch 49/1946, Loss: 0.2443, Time: 0.83s\n",
      "Batch 50/1946, Loss: 0.3118, Time: 0.83s\n",
      "Batch 51/1946, Loss: 0.2487, Time: 0.83s\n",
      "Batch 52/1946, Loss: 0.2799, Time: 0.83s\n",
      "Batch 53/1946, Loss: 0.2665, Time: 0.83s\n",
      "Batch 54/1946, Loss: 0.2696, Time: 0.83s\n",
      "Batch 55/1946, Loss: 0.2835, Time: 0.83s\n",
      "Batch 56/1946, Loss: 0.2404, Time: 0.82s\n",
      "Batch 57/1946, Loss: 0.2477, Time: 0.83s\n",
      "Batch 58/1946, Loss: 0.2308, Time: 0.82s\n",
      "Batch 59/1946, Loss: 0.2349, Time: 0.82s\n",
      "Batch 60/1946, Loss: 0.2177, Time: 0.83s\n",
      "Batch 61/1946, Loss: 0.2621, Time: 0.82s\n",
      "Batch 62/1946, Loss: 0.2109, Time: 0.83s\n",
      "Batch 63/1946, Loss: 0.2493, Time: 0.83s\n",
      "Batch 64/1946, Loss: 0.2541, Time: 0.83s\n",
      "Batch 65/1946, Loss: 0.2728, Time: 0.83s\n",
      "Batch 66/1946, Loss: 0.2904, Time: 0.82s\n",
      "Batch 67/1946, Loss: 0.2753, Time: 0.82s\n",
      "Batch 68/1946, Loss: 0.2417, Time: 0.83s\n",
      "Batch 69/1946, Loss: 0.2858, Time: 0.82s\n",
      "Batch 70/1946, Loss: 0.2558, Time: 0.82s\n",
      "Batch 71/1946, Loss: 0.2719, Time: 0.82s\n",
      "Batch 72/1946, Loss: 0.2092, Time: 0.82s\n",
      "Batch 73/1946, Loss: 0.2563, Time: 0.82s\n",
      "Batch 74/1946, Loss: 0.2948, Time: 0.82s\n",
      "Batch 75/1946, Loss: 0.3060, Time: 0.83s\n",
      "Batch 76/1946, Loss: 0.2364, Time: 0.82s\n",
      "Batch 77/1946, Loss: 0.2473, Time: 0.82s\n",
      "Batch 78/1946, Loss: 0.2309, Time: 0.82s\n",
      "Batch 79/1946, Loss: 0.2856, Time: 0.82s\n",
      "Batch 80/1946, Loss: 0.2748, Time: 0.83s\n",
      "Batch 81/1946, Loss: 0.2787, Time: 0.82s\n",
      "Batch 82/1946, Loss: 0.2479, Time: 0.82s\n",
      "Batch 83/1946, Loss: 0.2480, Time: 0.82s\n",
      "Batch 84/1946, Loss: 0.2390, Time: 0.83s\n",
      "Batch 85/1946, Loss: 0.2876, Time: 0.83s\n",
      "Batch 86/1946, Loss: 0.2614, Time: 0.83s\n",
      "Batch 87/1946, Loss: 0.2675, Time: 0.83s\n",
      "Batch 88/1946, Loss: 0.2358, Time: 0.83s\n",
      "Batch 89/1946, Loss: 0.2727, Time: 0.83s\n",
      "Batch 90/1946, Loss: 0.2657, Time: 0.83s\n",
      "Batch 91/1946, Loss: 0.2531, Time: 0.83s\n",
      "Batch 92/1946, Loss: 0.2792, Time: 0.83s\n",
      "Batch 93/1946, Loss: 0.2699, Time: 0.82s\n",
      "Batch 94/1946, Loss: 0.2565, Time: 0.83s\n",
      "Batch 95/1946, Loss: 0.2505, Time: 0.82s\n",
      "Batch 96/1946, Loss: 0.2630, Time: 0.82s\n",
      "Batch 97/1946, Loss: 0.2554, Time: 0.82s\n",
      "Batch 98/1946, Loss: 0.2367, Time: 0.82s\n",
      "Batch 99/1946, Loss: 0.2466, Time: 0.82s\n",
      "Batch 100/1946, Loss: 0.2570, Time: 0.82s\n",
      "Batch 101/1946, Loss: 0.2685, Time: 0.83s\n",
      "Batch 102/1946, Loss: 0.2467, Time: 0.83s\n",
      "Batch 103/1946, Loss: 0.2376, Time: 0.82s\n",
      "Batch 104/1946, Loss: 0.2910, Time: 0.82s\n",
      "Batch 105/1946, Loss: 0.2467, Time: 0.82s\n",
      "Batch 106/1946, Loss: 0.2458, Time: 0.82s\n",
      "Batch 107/1946, Loss: 0.2630, Time: 0.82s\n",
      "Batch 108/1946, Loss: 0.2219, Time: 0.82s\n",
      "Batch 109/1946, Loss: 0.2333, Time: 0.82s\n",
      "Batch 110/1946, Loss: 0.2390, Time: 0.82s\n",
      "Batch 111/1946, Loss: 0.2718, Time: 0.82s\n",
      "Batch 112/1946, Loss: 0.2225, Time: 0.82s\n",
      "Batch 113/1946, Loss: 0.2300, Time: 0.82s\n",
      "Batch 114/1946, Loss: 0.2404, Time: 0.82s\n",
      "Batch 115/1946, Loss: 0.2537, Time: 0.82s\n",
      "Batch 116/1946, Loss: 0.2221, Time: 0.82s\n",
      "Batch 117/1946, Loss: 0.2675, Time: 0.83s\n",
      "Batch 118/1946, Loss: 0.2559, Time: 0.83s\n",
      "Batch 119/1946, Loss: 0.2634, Time: 0.83s\n",
      "Batch 120/1946, Loss: 0.2486, Time: 0.83s\n",
      "Batch 121/1946, Loss: 0.2332, Time: 0.83s\n",
      "Batch 122/1946, Loss: 0.2760, Time: 0.83s\n",
      "Batch 123/1946, Loss: 0.2394, Time: 0.83s\n",
      "Batch 124/1946, Loss: 0.2664, Time: 0.83s\n",
      "Batch 125/1946, Loss: 0.2392, Time: 0.83s\n",
      "Batch 126/1946, Loss: 0.2394, Time: 0.82s\n",
      "Batch 127/1946, Loss: 0.2390, Time: 0.83s\n",
      "Batch 128/1946, Loss: 0.2669, Time: 0.83s\n",
      "Batch 129/1946, Loss: 0.2663, Time: 0.83s\n",
      "Batch 130/1946, Loss: 0.2551, Time: 0.83s\n",
      "Batch 131/1946, Loss: 0.2319, Time: 0.82s\n",
      "Batch 132/1946, Loss: 0.2771, Time: 0.82s\n",
      "Batch 133/1946, Loss: 0.2207, Time: 0.83s\n",
      "Batch 134/1946, Loss: 0.2086, Time: 0.83s\n",
      "Batch 135/1946, Loss: 0.2245, Time: 0.82s\n",
      "Batch 136/1946, Loss: 0.2411, Time: 0.82s\n",
      "Batch 137/1946, Loss: 0.2194, Time: 0.82s\n",
      "Batch 138/1946, Loss: 0.2434, Time: 0.83s\n",
      "Batch 139/1946, Loss: 0.2680, Time: 0.83s\n",
      "Batch 140/1946, Loss: 0.2318, Time: 0.82s\n",
      "Batch 141/1946, Loss: 0.2496, Time: 0.83s\n",
      "Batch 142/1946, Loss: 0.2648, Time: 0.82s\n",
      "Batch 143/1946, Loss: 0.2541, Time: 0.82s\n",
      "Batch 144/1946, Loss: 0.2990, Time: 0.83s\n",
      "Batch 145/1946, Loss: 0.2513, Time: 0.83s\n",
      "Batch 146/1946, Loss: 0.2373, Time: 0.83s\n",
      "Batch 147/1946, Loss: 0.2569, Time: 0.82s\n",
      "Batch 148/1946, Loss: 0.2327, Time: 0.83s\n",
      "Batch 149/1946, Loss: 0.2348, Time: 0.83s\n",
      "Batch 150/1946, Loss: 0.2153, Time: 0.83s\n",
      "Batch 151/1946, Loss: 0.2558, Time: 0.83s\n",
      "Batch 152/1946, Loss: 0.2373, Time: 0.83s\n",
      "Batch 153/1946, Loss: 0.2121, Time: 0.83s\n",
      "Batch 154/1946, Loss: 0.2248, Time: 0.83s\n",
      "Batch 155/1946, Loss: 0.2869, Time: 0.83s\n",
      "Batch 156/1946, Loss: 0.2396, Time: 0.82s\n",
      "Batch 157/1946, Loss: 0.2487, Time: 0.83s\n",
      "Batch 158/1946, Loss: 0.2253, Time: 0.83s\n",
      "Batch 159/1946, Loss: 0.2288, Time: 0.83s\n",
      "Batch 160/1946, Loss: 0.2484, Time: 0.83s\n",
      "Batch 161/1946, Loss: 0.2392, Time: 0.82s\n",
      "Batch 162/1946, Loss: 0.2489, Time: 0.83s\n",
      "Batch 163/1946, Loss: 0.2422, Time: 0.83s\n",
      "Batch 164/1946, Loss: 0.2279, Time: 0.83s\n",
      "Batch 165/1946, Loss: 0.2660, Time: 0.83s\n",
      "Batch 166/1946, Loss: 0.2539, Time: 0.83s\n",
      "Batch 167/1946, Loss: 0.2502, Time: 0.83s\n",
      "Batch 168/1946, Loss: 0.2976, Time: 0.83s\n",
      "Batch 169/1946, Loss: 0.2449, Time: 0.83s\n",
      "Batch 170/1946, Loss: 0.2450, Time: 0.83s\n",
      "Batch 171/1946, Loss: 0.2574, Time: 0.82s\n",
      "Batch 172/1946, Loss: 0.2199, Time: 0.83s\n",
      "Batch 173/1946, Loss: 0.2429, Time: 0.83s\n",
      "Batch 174/1946, Loss: 0.2263, Time: 0.83s\n",
      "Batch 175/1946, Loss: 0.2534, Time: 0.83s\n",
      "Batch 176/1946, Loss: 0.2625, Time: 0.82s\n",
      "Batch 177/1946, Loss: 0.2718, Time: 0.83s\n",
      "Batch 178/1946, Loss: 0.2079, Time: 0.83s\n",
      "Batch 179/1946, Loss: 0.2331, Time: 0.83s\n",
      "Batch 180/1946, Loss: 0.2754, Time: 0.82s\n",
      "Batch 181/1946, Loss: 0.2247, Time: 0.82s\n",
      "Batch 182/1946, Loss: 0.2445, Time: 0.83s\n",
      "Batch 183/1946, Loss: 0.2370, Time: 0.82s\n",
      "Batch 184/1946, Loss: 0.2383, Time: 0.82s\n",
      "Batch 185/1946, Loss: 0.2403, Time: 0.83s\n",
      "Batch 186/1946, Loss: 0.2132, Time: 0.82s\n",
      "Batch 187/1946, Loss: 0.2302, Time: 0.83s\n",
      "Batch 188/1946, Loss: 0.2533, Time: 0.82s\n",
      "Batch 189/1946, Loss: 0.2259, Time: 0.83s\n",
      "Batch 190/1946, Loss: 0.2772, Time: 0.83s\n",
      "Batch 191/1946, Loss: 0.2735, Time: 0.82s\n",
      "Batch 192/1946, Loss: 0.2458, Time: 0.82s\n",
      "Batch 193/1946, Loss: 0.2348, Time: 0.82s\n",
      "Batch 194/1946, Loss: 0.2241, Time: 0.82s\n",
      "Batch 195/1946, Loss: 0.2674, Time: 0.83s\n",
      "Batch 196/1946, Loss: 0.2184, Time: 0.82s\n",
      "Batch 197/1946, Loss: 0.2128, Time: 0.82s\n",
      "Batch 198/1946, Loss: 0.2341, Time: 0.83s\n",
      "Batch 199/1946, Loss: 0.2615, Time: 0.83s\n",
      "Batch 200/1946, Loss: 0.2232, Time: 0.83s\n",
      "Batch 201/1946, Loss: 0.2263, Time: 0.82s\n",
      "Batch 202/1946, Loss: 0.2100, Time: 0.83s\n",
      "Batch 203/1946, Loss: 0.2185, Time: 0.83s\n",
      "Batch 204/1946, Loss: 0.2504, Time: 0.82s\n",
      "Batch 205/1946, Loss: 0.2707, Time: 0.83s\n",
      "Batch 206/1946, Loss: 0.2307, Time: 0.83s\n",
      "Batch 207/1946, Loss: 0.2537, Time: 0.82s\n",
      "Batch 208/1946, Loss: 0.2732, Time: 0.82s\n",
      "Batch 209/1946, Loss: 0.2510, Time: 0.83s\n",
      "Batch 210/1946, Loss: 0.2739, Time: 0.83s\n",
      "Batch 211/1946, Loss: 0.2163, Time: 0.83s\n",
      "Batch 212/1946, Loss: 0.2754, Time: 0.83s\n",
      "Batch 213/1946, Loss: 0.2129, Time: 0.82s\n",
      "Batch 214/1946, Loss: 0.2711, Time: 0.83s\n",
      "Batch 215/1946, Loss: 0.2611, Time: 0.83s\n",
      "Batch 216/1946, Loss: 0.2572, Time: 0.82s\n",
      "Batch 217/1946, Loss: 0.2676, Time: 0.82s\n",
      "Batch 218/1946, Loss: 0.2371, Time: 0.83s\n",
      "Batch 219/1946, Loss: 0.1922, Time: 0.83s\n",
      "Batch 220/1946, Loss: 0.2236, Time: 0.83s\n",
      "Batch 221/1946, Loss: 0.3083, Time: 0.82s\n",
      "Batch 222/1946, Loss: 0.2440, Time: 0.83s\n",
      "Batch 223/1946, Loss: 0.2418, Time: 0.83s\n",
      "Batch 224/1946, Loss: 0.2445, Time: 0.82s\n",
      "Batch 225/1946, Loss: 0.2699, Time: 0.83s\n",
      "Batch 226/1946, Loss: 0.2298, Time: 0.82s\n",
      "Batch 227/1946, Loss: 0.2284, Time: 0.82s\n",
      "Batch 228/1946, Loss: 0.2348, Time: 0.82s\n",
      "Batch 229/1946, Loss: 0.2161, Time: 0.82s\n",
      "Batch 230/1946, Loss: 0.2435, Time: 0.82s\n",
      "Batch 231/1946, Loss: 0.2403, Time: 0.83s\n",
      "Batch 232/1946, Loss: 0.2566, Time: 0.83s\n",
      "Batch 233/1946, Loss: 0.2478, Time: 0.82s\n",
      "Batch 234/1946, Loss: 0.2140, Time: 0.83s\n",
      "Batch 235/1946, Loss: 0.2295, Time: 0.82s\n",
      "Batch 236/1946, Loss: 0.2333, Time: 0.82s\n",
      "Batch 237/1946, Loss: 0.2745, Time: 0.82s\n",
      "Batch 238/1946, Loss: 0.2054, Time: 0.82s\n",
      "Batch 239/1946, Loss: 0.2612, Time: 0.82s\n",
      "Batch 240/1946, Loss: 0.2266, Time: 0.82s\n",
      "Batch 241/1946, Loss: 0.2179, Time: 0.82s\n",
      "Batch 242/1946, Loss: 0.2120, Time: 0.82s\n",
      "Batch 243/1946, Loss: 0.2288, Time: 0.82s\n",
      "Batch 244/1946, Loss: 0.2147, Time: 0.82s\n",
      "Batch 245/1946, Loss: 0.2369, Time: 0.83s\n",
      "Batch 246/1946, Loss: 0.2109, Time: 0.82s\n",
      "Batch 247/1946, Loss: 0.2203, Time: 0.83s\n",
      "Batch 248/1946, Loss: 0.2289, Time: 0.83s\n",
      "Batch 249/1946, Loss: 0.2404, Time: 0.83s\n",
      "Batch 250/1946, Loss: 0.2365, Time: 0.83s\n",
      "Batch 251/1946, Loss: 0.2432, Time: 0.83s\n",
      "Batch 252/1946, Loss: 0.2328, Time: 0.82s\n",
      "Batch 253/1946, Loss: 0.2309, Time: 0.83s\n",
      "Batch 254/1946, Loss: 0.2671, Time: 0.82s\n",
      "Batch 255/1946, Loss: 0.2393, Time: 0.83s\n",
      "Batch 256/1946, Loss: 0.2784, Time: 0.82s\n",
      "Batch 257/1946, Loss: 0.2349, Time: 0.82s\n",
      "Batch 258/1946, Loss: 0.2356, Time: 0.83s\n",
      "Batch 259/1946, Loss: 0.2092, Time: 0.82s\n",
      "Batch 260/1946, Loss: 0.2483, Time: 0.82s\n",
      "Batch 261/1946, Loss: 0.2272, Time: 0.82s\n",
      "Batch 262/1946, Loss: 0.2169, Time: 0.82s\n",
      "Batch 263/1946, Loss: 0.2580, Time: 0.82s\n",
      "Batch 264/1946, Loss: 0.2418, Time: 0.82s\n",
      "Batch 265/1946, Loss: 0.2129, Time: 0.82s\n",
      "Batch 266/1946, Loss: 0.2232, Time: 0.83s\n",
      "Batch 267/1946, Loss: 0.2278, Time: 0.82s\n",
      "Batch 268/1946, Loss: 0.2571, Time: 0.83s\n",
      "Batch 269/1946, Loss: 0.2406, Time: 0.82s\n",
      "Batch 270/1946, Loss: 0.2349, Time: 0.82s\n",
      "Batch 271/1946, Loss: 0.2480, Time: 0.82s\n",
      "Batch 272/1946, Loss: 0.2414, Time: 0.82s\n",
      "Batch 273/1946, Loss: 0.2260, Time: 0.82s\n",
      "Batch 274/1946, Loss: 0.2376, Time: 0.82s\n",
      "Batch 275/1946, Loss: 0.2162, Time: 0.82s\n",
      "Batch 276/1946, Loss: 0.2513, Time: 0.82s\n",
      "Batch 277/1946, Loss: 0.2182, Time: 0.82s\n",
      "Batch 278/1946, Loss: 0.2422, Time: 0.82s\n",
      "Batch 279/1946, Loss: 0.2418, Time: 0.82s\n",
      "Batch 280/1946, Loss: 0.2460, Time: 0.82s\n",
      "Batch 281/1946, Loss: 0.2619, Time: 0.82s\n",
      "Batch 282/1946, Loss: 0.2661, Time: 0.82s\n",
      "Batch 283/1946, Loss: 0.2493, Time: 0.82s\n",
      "Batch 284/1946, Loss: 0.2286, Time: 0.82s\n",
      "Batch 285/1946, Loss: 0.2176, Time: 0.82s\n",
      "Batch 286/1946, Loss: 0.2411, Time: 0.82s\n",
      "Batch 287/1946, Loss: 0.1947, Time: 0.82s\n",
      "Batch 288/1946, Loss: 0.2249, Time: 0.82s\n",
      "Batch 289/1946, Loss: 0.2444, Time: 0.82s\n",
      "Batch 290/1946, Loss: 0.1926, Time: 0.82s\n",
      "Batch 291/1946, Loss: 0.3178, Time: 0.82s\n",
      "Batch 292/1946, Loss: 0.1985, Time: 0.83s\n",
      "Batch 293/1946, Loss: 0.2300, Time: 0.83s\n",
      "Batch 294/1946, Loss: 0.2180, Time: 0.82s\n",
      "Batch 295/1946, Loss: 0.2312, Time: 0.82s\n",
      "Batch 296/1946, Loss: 0.2655, Time: 0.82s\n",
      "Batch 297/1946, Loss: 0.2376, Time: 0.82s\n",
      "Batch 298/1946, Loss: 0.2412, Time: 0.82s\n",
      "Batch 299/1946, Loss: 0.2120, Time: 0.82s\n",
      "Batch 300/1946, Loss: 0.2188, Time: 0.82s\n",
      "Batch 301/1946, Loss: 0.2141, Time: 0.82s\n",
      "Batch 302/1946, Loss: 0.2559, Time: 0.82s\n",
      "Batch 303/1946, Loss: 0.2609, Time: 0.82s\n",
      "Batch 304/1946, Loss: 0.2037, Time: 0.82s\n",
      "Batch 305/1946, Loss: 0.2370, Time: 0.82s\n",
      "Batch 306/1946, Loss: 0.2256, Time: 0.82s\n",
      "Batch 307/1946, Loss: 0.2886, Time: 0.82s\n",
      "Batch 308/1946, Loss: 0.2258, Time: 0.82s\n",
      "Batch 309/1946, Loss: 0.2040, Time: 0.83s\n",
      "Batch 310/1946, Loss: 0.2272, Time: 0.82s\n",
      "Batch 311/1946, Loss: 0.2212, Time: 0.83s\n",
      "Batch 312/1946, Loss: 0.2262, Time: 0.83s\n",
      "Batch 313/1946, Loss: 0.2240, Time: 0.82s\n",
      "Batch 314/1946, Loss: 0.2368, Time: 0.83s\n",
      "Batch 315/1946, Loss: 0.2494, Time: 0.83s\n",
      "Batch 316/1946, Loss: 0.2314, Time: 0.83s\n",
      "Batch 317/1946, Loss: 0.1887, Time: 0.82s\n",
      "Batch 318/1946, Loss: 0.2097, Time: 0.83s\n",
      "Batch 319/1946, Loss: 0.2305, Time: 0.83s\n",
      "Batch 320/1946, Loss: 0.2289, Time: 0.83s\n",
      "Batch 321/1946, Loss: 0.2240, Time: 0.83s\n",
      "Batch 322/1946, Loss: 0.2330, Time: 0.83s\n",
      "Batch 323/1946, Loss: 0.2445, Time: 0.83s\n",
      "Batch 324/1946, Loss: 0.2447, Time: 0.83s\n",
      "Batch 325/1946, Loss: 0.2222, Time: 0.82s\n",
      "Batch 326/1946, Loss: 0.2453, Time: 0.83s\n",
      "Batch 327/1946, Loss: 0.2145, Time: 0.82s\n",
      "Batch 328/1946, Loss: 0.2700, Time: 0.82s\n",
      "Batch 329/1946, Loss: 0.2052, Time: 0.83s\n",
      "Batch 330/1946, Loss: 0.2086, Time: 0.83s\n",
      "Batch 331/1946, Loss: 0.2434, Time: 0.83s\n",
      "Batch 332/1946, Loss: 0.2597, Time: 0.83s\n",
      "Batch 333/1946, Loss: 0.2558, Time: 0.82s\n",
      "Batch 334/1946, Loss: 0.1887, Time: 0.83s\n",
      "Batch 335/1946, Loss: 0.2123, Time: 0.82s\n",
      "Batch 336/1946, Loss: 0.2679, Time: 0.82s\n",
      "Batch 337/1946, Loss: 0.2083, Time: 0.82s\n",
      "Batch 338/1946, Loss: 0.2084, Time: 0.83s\n",
      "Batch 339/1946, Loss: 0.1985, Time: 0.82s\n",
      "Batch 340/1946, Loss: 0.2385, Time: 0.82s\n",
      "Batch 341/1946, Loss: 0.2430, Time: 0.82s\n",
      "Batch 342/1946, Loss: 0.1871, Time: 0.83s\n",
      "Batch 343/1946, Loss: 0.2160, Time: 0.82s\n",
      "Batch 344/1946, Loss: 0.2424, Time: 0.83s\n",
      "Batch 345/1946, Loss: 0.2200, Time: 0.83s\n",
      "Batch 346/1946, Loss: 0.2147, Time: 0.83s\n",
      "Batch 347/1946, Loss: 0.2376, Time: 0.82s\n",
      "Batch 348/1946, Loss: 0.2197, Time: 0.83s\n",
      "Batch 349/1946, Loss: 0.1983, Time: 0.83s\n",
      "Batch 350/1946, Loss: 0.2445, Time: 0.83s\n",
      "Batch 351/1946, Loss: 0.2434, Time: 0.83s\n",
      "Batch 352/1946, Loss: 0.2482, Time: 0.82s\n",
      "Batch 353/1946, Loss: 0.2492, Time: 0.82s\n",
      "Batch 354/1946, Loss: 0.2238, Time: 0.82s\n",
      "Batch 355/1946, Loss: 0.1989, Time: 0.82s\n",
      "Batch 356/1946, Loss: 0.2268, Time: 0.82s\n",
      "Batch 357/1946, Loss: 0.2467, Time: 0.82s\n",
      "Batch 358/1946, Loss: 0.2323, Time: 0.82s\n",
      "Batch 359/1946, Loss: 0.2289, Time: 0.83s\n",
      "Batch 360/1946, Loss: 0.1878, Time: 0.82s\n",
      "Batch 361/1946, Loss: 0.2585, Time: 0.82s\n",
      "Batch 362/1946, Loss: 0.2051, Time: 0.83s\n",
      "Batch 363/1946, Loss: 0.2131, Time: 0.83s\n",
      "Batch 364/1946, Loss: 0.2603, Time: 0.83s\n",
      "Batch 365/1946, Loss: 0.2110, Time: 0.82s\n",
      "Batch 366/1946, Loss: 0.2301, Time: 0.83s\n",
      "Batch 367/1946, Loss: 0.2921, Time: 0.83s\n",
      "Batch 368/1946, Loss: 0.2409, Time: 0.83s\n",
      "Batch 369/1946, Loss: 0.2229, Time: 0.83s\n",
      "Batch 370/1946, Loss: 0.2350, Time: 0.82s\n",
      "Batch 371/1946, Loss: 0.2299, Time: 0.83s\n",
      "Batch 372/1946, Loss: 0.2067, Time: 0.83s\n",
      "Batch 373/1946, Loss: 0.2676, Time: 0.82s\n",
      "Batch 374/1946, Loss: 0.2784, Time: 0.82s\n",
      "Batch 375/1946, Loss: 0.1975, Time: 0.82s\n",
      "Batch 376/1946, Loss: 0.2179, Time: 0.82s\n",
      "Batch 377/1946, Loss: 0.2667, Time: 0.82s\n",
      "Batch 378/1946, Loss: 0.2604, Time: 0.82s\n",
      "Batch 379/1946, Loss: 0.2224, Time: 0.82s\n",
      "Batch 380/1946, Loss: 0.2223, Time: 0.82s\n",
      "Batch 381/1946, Loss: 0.2215, Time: 0.82s\n",
      "Batch 382/1946, Loss: 0.2799, Time: 0.82s\n",
      "Batch 383/1946, Loss: 0.2368, Time: 0.82s\n",
      "Batch 384/1946, Loss: 0.2258, Time: 0.83s\n",
      "Batch 385/1946, Loss: 0.2415, Time: 0.83s\n",
      "Batch 386/1946, Loss: 0.2842, Time: 0.82s\n",
      "Batch 387/1946, Loss: 0.2188, Time: 0.82s\n",
      "Batch 388/1946, Loss: 0.2432, Time: 0.82s\n",
      "Batch 389/1946, Loss: 0.2203, Time: 0.82s\n",
      "Batch 390/1946, Loss: 0.2035, Time: 0.83s\n",
      "Batch 391/1946, Loss: 0.2272, Time: 0.82s\n",
      "Batch 392/1946, Loss: 0.1994, Time: 0.83s\n",
      "Batch 393/1946, Loss: 0.2379, Time: 0.83s\n",
      "Batch 394/1946, Loss: 0.2056, Time: 0.83s\n",
      "Batch 395/1946, Loss: 0.2411, Time: 0.83s\n",
      "Batch 396/1946, Loss: 0.2098, Time: 0.82s\n",
      "Batch 397/1946, Loss: 0.2446, Time: 0.83s\n",
      "Batch 398/1946, Loss: 0.2579, Time: 0.83s\n",
      "Batch 399/1946, Loss: 0.2252, Time: 0.83s\n",
      "Batch 400/1946, Loss: 0.2386, Time: 0.83s\n",
      "Batch 401/1946, Loss: 0.2473, Time: 0.82s\n",
      "Batch 402/1946, Loss: 0.2134, Time: 0.83s\n",
      "Batch 403/1946, Loss: 0.1960, Time: 0.83s\n",
      "Batch 404/1946, Loss: 0.2285, Time: 0.83s\n",
      "Batch 405/1946, Loss: 0.1929, Time: 0.83s\n",
      "Batch 406/1946, Loss: 0.2242, Time: 0.82s\n",
      "Batch 407/1946, Loss: 0.2088, Time: 0.83s\n",
      "Batch 408/1946, Loss: 0.1930, Time: 0.83s\n",
      "Batch 409/1946, Loss: 0.2561, Time: 0.83s\n",
      "Batch 410/1946, Loss: 0.2105, Time: 0.83s\n",
      "Batch 411/1946, Loss: 0.2079, Time: 0.82s\n",
      "Batch 412/1946, Loss: 0.2679, Time: 0.83s\n",
      "Batch 413/1946, Loss: 0.2500, Time: 0.83s\n",
      "Batch 414/1946, Loss: 0.2242, Time: 0.83s\n",
      "Batch 415/1946, Loss: 0.2144, Time: 0.83s\n",
      "Batch 416/1946, Loss: 0.2410, Time: 0.82s\n",
      "Batch 417/1946, Loss: 0.2509, Time: 0.83s\n",
      "Batch 418/1946, Loss: 0.2592, Time: 0.83s\n",
      "Batch 419/1946, Loss: 0.2393, Time: 0.83s\n",
      "Batch 420/1946, Loss: 0.2492, Time: 0.83s\n",
      "Batch 421/1946, Loss: 0.2287, Time: 0.82s\n",
      "Batch 422/1946, Loss: 0.2211, Time: 0.83s\n",
      "Batch 423/1946, Loss: 0.2424, Time: 0.83s\n",
      "Batch 424/1946, Loss: 0.2598, Time: 0.83s\n",
      "Batch 425/1946, Loss: 0.2318, Time: 0.83s\n",
      "Batch 426/1946, Loss: 0.2122, Time: 0.82s\n",
      "Batch 427/1946, Loss: 0.2298, Time: 0.83s\n",
      "Batch 428/1946, Loss: 0.2407, Time: 0.83s\n",
      "Batch 429/1946, Loss: 0.1951, Time: 0.83s\n",
      "Batch 430/1946, Loss: 0.2184, Time: 0.83s\n",
      "Batch 431/1946, Loss: 0.2144, Time: 0.82s\n",
      "Batch 432/1946, Loss: 0.2125, Time: 0.82s\n",
      "Batch 433/1946, Loss: 0.2417, Time: 0.82s\n",
      "Batch 434/1946, Loss: 0.2042, Time: 0.82s\n",
      "Batch 435/1946, Loss: 0.2339, Time: 0.82s\n",
      "Batch 436/1946, Loss: 0.2224, Time: 0.83s\n",
      "Batch 437/1946, Loss: 0.2295, Time: 0.83s\n",
      "Batch 438/1946, Loss: 0.2615, Time: 0.83s\n",
      "Batch 439/1946, Loss: 0.1973, Time: 0.82s\n",
      "Batch 440/1946, Loss: 0.2326, Time: 0.83s\n",
      "Batch 441/1946, Loss: 0.2173, Time: 0.83s\n",
      "Batch 442/1946, Loss: 0.2349, Time: 0.82s\n",
      "Batch 443/1946, Loss: 0.2496, Time: 0.83s\n",
      "Batch 444/1946, Loss: 0.2350, Time: 0.83s\n",
      "Batch 445/1946, Loss: 0.2305, Time: 0.83s\n",
      "Batch 446/1946, Loss: 0.2198, Time: 0.83s\n",
      "Batch 447/1946, Loss: 0.2350, Time: 0.83s\n",
      "Batch 448/1946, Loss: 0.1934, Time: 0.83s\n",
      "Batch 449/1946, Loss: 0.2292, Time: 0.83s\n",
      "Batch 450/1946, Loss: 0.2493, Time: 0.83s\n",
      "Batch 451/1946, Loss: 0.1855, Time: 0.83s\n",
      "Batch 452/1946, Loss: 0.2131, Time: 0.83s\n",
      "Batch 453/1946, Loss: 0.1784, Time: 0.83s\n",
      "Batch 454/1946, Loss: 0.2101, Time: 0.83s\n",
      "Batch 455/1946, Loss: 0.1947, Time: 0.82s\n",
      "Batch 456/1946, Loss: 0.2070, Time: 0.82s\n",
      "Batch 457/1946, Loss: 0.1744, Time: 0.82s\n",
      "Batch 458/1946, Loss: 0.1976, Time: 0.82s\n",
      "Batch 459/1946, Loss: 0.2123, Time: 0.82s\n",
      "Batch 460/1946, Loss: 0.2226, Time: 0.82s\n",
      "Batch 461/1946, Loss: 0.3267, Time: 0.83s\n",
      "Batch 462/1946, Loss: 0.2180, Time: 0.82s\n",
      "Batch 463/1946, Loss: 0.2513, Time: 0.82s\n",
      "Batch 464/1946, Loss: 0.2150, Time: 0.82s\n",
      "Batch 465/1946, Loss: 0.2549, Time: 0.82s\n",
      "Batch 466/1946, Loss: 0.2375, Time: 0.82s\n",
      "Batch 467/1946, Loss: 0.2789, Time: 0.82s\n",
      "Batch 468/1946, Loss: 0.2397, Time: 0.82s\n",
      "Batch 469/1946, Loss: 0.2508, Time: 0.82s\n",
      "Batch 470/1946, Loss: 0.2247, Time: 0.82s\n",
      "Batch 471/1946, Loss: 0.2504, Time: 0.82s\n",
      "Batch 472/1946, Loss: 0.2506, Time: 0.82s\n",
      "Batch 473/1946, Loss: 0.2510, Time: 0.82s\n",
      "Batch 474/1946, Loss: 0.2220, Time: 0.82s\n",
      "Batch 475/1946, Loss: 0.2015, Time: 0.82s\n",
      "Batch 476/1946, Loss: 0.2487, Time: 0.82s\n",
      "Batch 477/1946, Loss: 0.2633, Time: 0.83s\n",
      "Batch 478/1946, Loss: 0.1948, Time: 0.82s\n",
      "Batch 479/1946, Loss: 0.1981, Time: 0.82s\n",
      "Batch 480/1946, Loss: 0.2421, Time: 0.82s\n",
      "Batch 481/1946, Loss: 0.2428, Time: 0.82s\n",
      "Batch 482/1946, Loss: 0.2200, Time: 0.82s\n",
      "Batch 483/1946, Loss: 0.2641, Time: 0.83s\n",
      "Batch 484/1946, Loss: 0.2442, Time: 0.82s\n",
      "Batch 485/1946, Loss: 0.2110, Time: 0.83s\n",
      "Batch 486/1946, Loss: 0.2478, Time: 0.82s\n",
      "Batch 487/1946, Loss: 0.2668, Time: 0.83s\n",
      "Batch 488/1946, Loss: 0.2100, Time: 0.83s\n",
      "Batch 489/1946, Loss: 0.2763, Time: 0.82s\n",
      "Batch 490/1946, Loss: 0.1945, Time: 0.82s\n",
      "Batch 491/1946, Loss: 0.1759, Time: 0.82s\n",
      "Batch 492/1946, Loss: 0.2283, Time: 0.82s\n",
      "Batch 493/1946, Loss: 0.2111, Time: 0.82s\n",
      "Batch 494/1946, Loss: 0.1678, Time: 0.82s\n",
      "Batch 495/1946, Loss: 0.2164, Time: 0.82s\n",
      "Batch 496/1946, Loss: 0.2051, Time: 0.82s\n",
      "Batch 497/1946, Loss: 0.2309, Time: 0.82s\n",
      "Batch 498/1946, Loss: 0.1909, Time: 0.83s\n",
      "Batch 499/1946, Loss: 0.2476, Time: 0.83s\n",
      "Batch 500/1946, Loss: 0.2366, Time: 0.83s\n",
      "Batch 501/1946, Loss: 0.2725, Time: 0.83s\n",
      "Batch 502/1946, Loss: 0.2062, Time: 0.83s\n",
      "Batch 503/1946, Loss: 0.2381, Time: 0.83s\n",
      "Batch 504/1946, Loss: 0.1756, Time: 0.83s\n",
      "Batch 505/1946, Loss: 0.1951, Time: 0.83s\n",
      "Batch 506/1946, Loss: 0.2153, Time: 0.82s\n",
      "Batch 507/1946, Loss: 0.2036, Time: 0.82s\n",
      "Batch 508/1946, Loss: 0.2449, Time: 0.83s\n",
      "Batch 509/1946, Loss: 0.1826, Time: 0.83s\n",
      "Batch 510/1946, Loss: 0.2340, Time: 0.83s\n",
      "Batch 511/1946, Loss: 0.2373, Time: 0.83s\n",
      "Batch 512/1946, Loss: 0.1982, Time: 0.83s\n",
      "Batch 513/1946, Loss: 0.2447, Time: 0.83s\n",
      "Batch 514/1946, Loss: 0.2235, Time: 0.83s\n",
      "Batch 515/1946, Loss: 0.2097, Time: 0.83s\n",
      "Batch 516/1946, Loss: 0.2251, Time: 0.83s\n",
      "Batch 517/1946, Loss: 0.2076, Time: 0.82s\n",
      "Batch 518/1946, Loss: 0.2234, Time: 0.82s\n",
      "Batch 519/1946, Loss: 0.2029, Time: 0.83s\n",
      "Batch 520/1946, Loss: 0.2386, Time: 0.82s\n",
      "Batch 521/1946, Loss: 0.1865, Time: 0.82s\n",
      "Batch 522/1946, Loss: 0.2327, Time: 0.83s\n",
      "Batch 523/1946, Loss: 0.2242, Time: 0.82s\n",
      "Batch 524/1946, Loss: 0.2102, Time: 0.82s\n",
      "Batch 525/1946, Loss: 0.2342, Time: 0.82s\n",
      "Batch 526/1946, Loss: 0.2222, Time: 0.82s\n",
      "Batch 527/1946, Loss: 0.2577, Time: 0.83s\n",
      "Batch 528/1946, Loss: 0.2050, Time: 0.83s\n",
      "Batch 529/1946, Loss: 0.2304, Time: 0.83s\n",
      "Batch 530/1946, Loss: 0.2113, Time: 0.82s\n",
      "Batch 531/1946, Loss: 0.1956, Time: 0.82s\n",
      "Batch 532/1946, Loss: 0.2747, Time: 0.82s\n",
      "Batch 533/1946, Loss: 0.2071, Time: 0.82s\n",
      "Batch 534/1946, Loss: 0.2304, Time: 0.82s\n",
      "Batch 535/1946, Loss: 0.1959, Time: 0.82s\n",
      "Batch 536/1946, Loss: 0.2239, Time: 0.82s\n",
      "Batch 537/1946, Loss: 0.2441, Time: 0.82s\n",
      "Batch 538/1946, Loss: 0.2034, Time: 0.82s\n",
      "Batch 539/1946, Loss: 0.2140, Time: 0.82s\n",
      "Batch 540/1946, Loss: 0.2701, Time: 0.83s\n",
      "Batch 541/1946, Loss: 0.2335, Time: 0.82s\n",
      "Batch 542/1946, Loss: 0.1969, Time: 0.83s\n",
      "Batch 543/1946, Loss: 0.2345, Time: 0.82s\n",
      "Batch 544/1946, Loss: 0.2073, Time: 0.82s\n",
      "Batch 545/1946, Loss: 0.2924, Time: 0.82s\n",
      "Batch 546/1946, Loss: 0.1824, Time: 0.82s\n",
      "Batch 547/1946, Loss: 0.2035, Time: 0.83s\n",
      "Batch 548/1946, Loss: 0.2343, Time: 0.82s\n",
      "Batch 549/1946, Loss: 0.2550, Time: 0.82s\n",
      "Batch 550/1946, Loss: 0.2324, Time: 0.82s\n",
      "Batch 551/1946, Loss: 0.1925, Time: 0.83s\n",
      "Batch 552/1946, Loss: 0.2087, Time: 0.82s\n",
      "Batch 553/1946, Loss: 0.2420, Time: 0.83s\n",
      "Batch 554/1946, Loss: 0.2161, Time: 0.82s\n",
      "Batch 555/1946, Loss: 0.1848, Time: 0.83s\n",
      "Batch 556/1946, Loss: 0.2707, Time: 0.83s\n",
      "Batch 557/1946, Loss: 0.2242, Time: 0.82s\n",
      "Batch 558/1946, Loss: 0.2022, Time: 0.82s\n",
      "Batch 559/1946, Loss: 0.2175, Time: 0.82s\n",
      "Batch 560/1946, Loss: 0.2335, Time: 0.82s\n",
      "Batch 561/1946, Loss: 0.1995, Time: 0.82s\n",
      "Batch 562/1946, Loss: 0.1967, Time: 0.82s\n",
      "Batch 563/1946, Loss: 0.2170, Time: 0.82s\n",
      "Batch 564/1946, Loss: 0.2053, Time: 0.82s\n",
      "Batch 565/1946, Loss: 0.2020, Time: 0.82s\n",
      "Batch 566/1946, Loss: 0.1865, Time: 0.83s\n",
      "Batch 567/1946, Loss: 0.2344, Time: 0.82s\n",
      "Batch 568/1946, Loss: 0.2223, Time: 0.83s\n",
      "Batch 569/1946, Loss: 0.2641, Time: 0.82s\n",
      "Batch 570/1946, Loss: 0.2052, Time: 0.82s\n",
      "Batch 571/1946, Loss: 0.1776, Time: 0.82s\n",
      "Batch 572/1946, Loss: 0.2584, Time: 0.82s\n",
      "Batch 573/1946, Loss: 0.2594, Time: 0.82s\n",
      "Batch 574/1946, Loss: 0.2259, Time: 0.82s\n",
      "Batch 575/1946, Loss: 0.2471, Time: 0.82s\n",
      "Batch 576/1946, Loss: 0.2476, Time: 0.82s\n",
      "Batch 577/1946, Loss: 0.2414, Time: 0.82s\n",
      "Batch 578/1946, Loss: 0.2164, Time: 0.82s\n",
      "Batch 579/1946, Loss: 0.2411, Time: 0.83s\n",
      "Batch 580/1946, Loss: 0.2572, Time: 0.83s\n",
      "Batch 581/1946, Loss: 0.2617, Time: 0.83s\n",
      "Batch 582/1946, Loss: 0.2180, Time: 0.83s\n",
      "Batch 583/1946, Loss: 0.2110, Time: 0.82s\n",
      "Batch 584/1946, Loss: 0.2347, Time: 0.82s\n",
      "Batch 585/1946, Loss: 0.2013, Time: 0.82s\n",
      "Batch 586/1946, Loss: 0.2605, Time: 0.82s\n",
      "Batch 587/1946, Loss: 0.2267, Time: 0.83s\n",
      "Batch 588/1946, Loss: 0.2394, Time: 0.82s\n",
      "Batch 589/1946, Loss: 0.2244, Time: 0.82s\n",
      "Batch 590/1946, Loss: 0.2196, Time: 0.83s\n",
      "Batch 591/1946, Loss: 0.1990, Time: 0.82s\n",
      "Batch 592/1946, Loss: 0.2154, Time: 0.82s\n",
      "Batch 593/1946, Loss: 0.2722, Time: 0.82s\n",
      "Batch 594/1946, Loss: 0.2178, Time: 0.82s\n",
      "Batch 595/1946, Loss: 0.2333, Time: 0.82s\n",
      "Batch 596/1946, Loss: 0.2602, Time: 0.83s\n",
      "Batch 597/1946, Loss: 0.2254, Time: 0.82s\n",
      "Batch 598/1946, Loss: 0.2365, Time: 0.83s\n",
      "Batch 599/1946, Loss: 0.2056, Time: 0.82s\n",
      "Batch 600/1946, Loss: 0.2060, Time: 0.82s\n",
      "Batch 601/1946, Loss: 0.2272, Time: 0.82s\n",
      "Batch 602/1946, Loss: 0.2338, Time: 0.82s\n",
      "Batch 603/1946, Loss: 0.2323, Time: 0.83s\n",
      "Batch 604/1946, Loss: 0.2502, Time: 0.82s\n",
      "Batch 605/1946, Loss: 0.1921, Time: 0.83s\n",
      "Batch 606/1946, Loss: 0.2220, Time: 0.82s\n",
      "Batch 607/1946, Loss: 0.2308, Time: 0.82s\n",
      "Batch 608/1946, Loss: 0.1866, Time: 0.82s\n",
      "Batch 609/1946, Loss: 0.2069, Time: 0.82s\n",
      "Batch 610/1946, Loss: 0.2231, Time: 0.82s\n",
      "Batch 611/1946, Loss: 0.2192, Time: 0.82s\n",
      "Batch 612/1946, Loss: 0.2271, Time: 0.82s\n",
      "Batch 613/1946, Loss: 0.2053, Time: 0.82s\n",
      "Batch 614/1946, Loss: 0.2520, Time: 0.82s\n",
      "Batch 615/1946, Loss: 0.2366, Time: 0.82s\n",
      "Batch 616/1946, Loss: 0.2244, Time: 0.83s\n",
      "Batch 617/1946, Loss: 0.2048, Time: 0.82s\n",
      "Batch 618/1946, Loss: 0.2327, Time: 0.82s\n",
      "Batch 619/1946, Loss: 0.2508, Time: 0.83s\n",
      "Batch 620/1946, Loss: 0.2300, Time: 0.82s\n",
      "Batch 621/1946, Loss: 0.2244, Time: 0.82s\n",
      "Batch 622/1946, Loss: 0.2028, Time: 0.82s\n",
      "Batch 623/1946, Loss: 0.2262, Time: 0.83s\n",
      "Batch 624/1946, Loss: 0.2484, Time: 0.83s\n",
      "Batch 625/1946, Loss: 0.2409, Time: 0.82s\n",
      "Batch 626/1946, Loss: 0.1914, Time: 0.82s\n",
      "Batch 627/1946, Loss: 0.2137, Time: 0.82s\n",
      "Batch 628/1946, Loss: 0.1906, Time: 0.82s\n",
      "Batch 629/1946, Loss: 0.2268, Time: 0.83s\n",
      "Batch 630/1946, Loss: 0.2454, Time: 0.83s\n",
      "Batch 631/1946, Loss: 0.2272, Time: 0.83s\n",
      "Batch 632/1946, Loss: 0.2350, Time: 0.83s\n",
      "Batch 633/1946, Loss: 0.3016, Time: 0.83s\n",
      "Batch 634/1946, Loss: 0.2018, Time: 0.83s\n",
      "Batch 635/1946, Loss: 0.2437, Time: 0.83s\n",
      "Batch 636/1946, Loss: 0.2352, Time: 0.82s\n",
      "Batch 637/1946, Loss: 0.2184, Time: 0.82s\n",
      "Batch 638/1946, Loss: 0.2793, Time: 0.82s\n",
      "Batch 639/1946, Loss: 0.2051, Time: 0.82s\n",
      "Batch 640/1946, Loss: 0.2162, Time: 0.82s\n",
      "Batch 641/1946, Loss: 0.2166, Time: 0.82s\n",
      "Batch 642/1946, Loss: 0.2568, Time: 0.82s\n",
      "Batch 643/1946, Loss: 0.1974, Time: 0.82s\n",
      "Batch 644/1946, Loss: 0.2335, Time: 0.82s\n",
      "Batch 645/1946, Loss: 0.2326, Time: 0.82s\n",
      "Batch 646/1946, Loss: 0.2531, Time: 0.83s\n",
      "Batch 647/1946, Loss: 0.2047, Time: 0.82s\n",
      "Batch 648/1946, Loss: 0.2205, Time: 0.83s\n",
      "Batch 649/1946, Loss: 0.2218, Time: 0.83s\n",
      "Batch 650/1946, Loss: 0.2082, Time: 0.83s\n",
      "Batch 651/1946, Loss: 0.1900, Time: 0.82s\n",
      "Batch 652/1946, Loss: 0.2236, Time: 0.83s\n",
      "Batch 653/1946, Loss: 0.2088, Time: 0.83s\n",
      "Batch 654/1946, Loss: 0.2330, Time: 0.83s\n",
      "Batch 655/1946, Loss: 0.2392, Time: 0.82s\n",
      "Batch 656/1946, Loss: 0.2662, Time: 0.83s\n",
      "Batch 657/1946, Loss: 0.2457, Time: 0.82s\n",
      "Batch 658/1946, Loss: 0.2037, Time: 0.83s\n",
      "Batch 659/1946, Loss: 0.2295, Time: 0.82s\n",
      "Batch 660/1946, Loss: 0.2454, Time: 0.82s\n",
      "Batch 661/1946, Loss: 0.2091, Time: 0.83s\n",
      "Batch 662/1946, Loss: 0.2082, Time: 0.82s\n",
      "Batch 663/1946, Loss: 0.2377, Time: 0.82s\n",
      "Batch 664/1946, Loss: 0.2053, Time: 0.82s\n",
      "Batch 665/1946, Loss: 0.2336, Time: 0.83s\n",
      "Batch 666/1946, Loss: 0.2078, Time: 0.82s\n",
      "Batch 667/1946, Loss: 0.2154, Time: 0.82s\n",
      "Batch 668/1946, Loss: 0.2130, Time: 0.83s\n",
      "Batch 669/1946, Loss: 0.2217, Time: 0.82s\n",
      "Batch 670/1946, Loss: 0.2595, Time: 0.82s\n",
      "Batch 671/1946, Loss: 0.1850, Time: 0.82s\n",
      "Batch 672/1946, Loss: 0.2230, Time: 0.82s\n",
      "Batch 673/1946, Loss: 0.2193, Time: 0.82s\n",
      "Batch 674/1946, Loss: 0.2576, Time: 0.82s\n",
      "Batch 675/1946, Loss: 0.1935, Time: 0.82s\n",
      "Batch 676/1946, Loss: 0.2118, Time: 0.82s\n",
      "Batch 677/1946, Loss: 0.2374, Time: 0.82s\n",
      "Batch 678/1946, Loss: 0.2161, Time: 0.82s\n",
      "Batch 679/1946, Loss: 0.2413, Time: 0.82s\n",
      "Batch 680/1946, Loss: 0.2065, Time: 0.82s\n",
      "Batch 681/1946, Loss: 0.1959, Time: 0.82s\n",
      "Batch 682/1946, Loss: 0.2675, Time: 0.82s\n",
      "Batch 683/1946, Loss: 0.2391, Time: 0.82s\n",
      "Batch 684/1946, Loss: 0.2290, Time: 0.82s\n",
      "Batch 685/1946, Loss: 0.2490, Time: 0.82s\n",
      "Batch 686/1946, Loss: 0.2331, Time: 0.82s\n",
      "Batch 687/1946, Loss: 0.2504, Time: 0.82s\n",
      "Batch 688/1946, Loss: 0.2135, Time: 0.83s\n",
      "Batch 689/1946, Loss: 0.2351, Time: 0.82s\n",
      "Batch 690/1946, Loss: 0.2729, Time: 0.83s\n",
      "Batch 691/1946, Loss: 0.2403, Time: 0.83s\n",
      "Batch 692/1946, Loss: 0.2146, Time: 0.83s\n",
      "Batch 693/1946, Loss: 0.2528, Time: 0.83s\n",
      "Batch 694/1946, Loss: 0.2630, Time: 0.82s\n",
      "Batch 695/1946, Loss: 0.2505, Time: 0.82s\n",
      "Batch 696/1946, Loss: 0.2091, Time: 0.82s\n",
      "Batch 697/1946, Loss: 0.1981, Time: 0.82s\n",
      "Batch 698/1946, Loss: 0.2265, Time: 0.82s\n",
      "Batch 699/1946, Loss: 0.2009, Time: 0.82s\n",
      "Batch 700/1946, Loss: 0.2488, Time: 0.82s\n",
      "Batch 701/1946, Loss: 0.2168, Time: 0.82s\n",
      "Batch 702/1946, Loss: 0.1978, Time: 0.82s\n",
      "Batch 703/1946, Loss: 0.2831, Time: 0.83s\n",
      "Batch 704/1946, Loss: 0.2331, Time: 0.82s\n",
      "Batch 705/1946, Loss: 0.2039, Time: 0.82s\n",
      "Batch 706/1946, Loss: 0.2130, Time: 0.82s\n",
      "Batch 707/1946, Loss: 0.2808, Time: 0.82s\n",
      "Batch 708/1946, Loss: 0.2071, Time: 0.82s\n",
      "Batch 709/1946, Loss: 0.1988, Time: 0.82s\n",
      "Batch 710/1946, Loss: 0.2487, Time: 0.82s\n",
      "Batch 711/1946, Loss: 0.2403, Time: 0.82s\n",
      "Batch 712/1946, Loss: 0.2090, Time: 0.82s\n",
      "Batch 713/1946, Loss: 0.2499, Time: 0.82s\n",
      "Batch 714/1946, Loss: 0.1851, Time: 0.82s\n",
      "Batch 715/1946, Loss: 0.2521, Time: 0.82s\n",
      "Batch 716/1946, Loss: 0.1939, Time: 0.83s\n",
      "Batch 717/1946, Loss: 0.2490, Time: 0.83s\n",
      "Batch 718/1946, Loss: 0.2288, Time: 0.83s\n",
      "Batch 719/1946, Loss: 0.2266, Time: 0.83s\n",
      "Batch 720/1946, Loss: 0.2170, Time: 0.82s\n",
      "Batch 721/1946, Loss: 0.2109, Time: 0.83s\n",
      "Batch 722/1946, Loss: 0.2373, Time: 0.83s\n",
      "Batch 723/1946, Loss: 0.2021, Time: 0.83s\n",
      "Batch 724/1946, Loss: 0.2430, Time: 0.83s\n",
      "Batch 725/1946, Loss: 0.2675, Time: 0.82s\n",
      "Batch 726/1946, Loss: 0.2225, Time: 0.82s\n",
      "Batch 727/1946, Loss: 0.2352, Time: 0.83s\n",
      "Batch 728/1946, Loss: 0.2375, Time: 0.82s\n",
      "Batch 729/1946, Loss: 0.2049, Time: 0.82s\n",
      "Batch 730/1946, Loss: 0.2199, Time: 0.82s\n",
      "Batch 731/1946, Loss: 0.1859, Time: 0.83s\n",
      "Batch 732/1946, Loss: 0.2009, Time: 0.83s\n",
      "Batch 733/1946, Loss: 0.1777, Time: 0.83s\n",
      "Batch 734/1946, Loss: 0.2141, Time: 0.83s\n",
      "Batch 735/1946, Loss: 0.1865, Time: 0.82s\n",
      "Batch 736/1946, Loss: 0.1952, Time: 0.82s\n",
      "Batch 737/1946, Loss: 0.2191, Time: 0.82s\n",
      "Batch 738/1946, Loss: 0.2420, Time: 0.82s\n",
      "Batch 739/1946, Loss: 0.2073, Time: 0.82s\n",
      "Batch 740/1946, Loss: 0.1954, Time: 0.82s\n",
      "Batch 741/1946, Loss: 0.2175, Time: 0.82s\n",
      "Batch 742/1946, Loss: 0.2324, Time: 0.83s\n",
      "Batch 743/1946, Loss: 0.2368, Time: 0.83s\n",
      "Batch 744/1946, Loss: 0.1982, Time: 0.83s\n",
      "Batch 745/1946, Loss: 0.2540, Time: 0.83s\n",
      "Batch 746/1946, Loss: 0.2195, Time: 0.82s\n",
      "Batch 747/1946, Loss: 0.2312, Time: 0.82s\n",
      "Batch 748/1946, Loss: 0.2427, Time: 0.82s\n",
      "Batch 749/1946, Loss: 0.2354, Time: 0.82s\n",
      "Batch 750/1946, Loss: 0.2111, Time: 0.82s\n",
      "Batch 751/1946, Loss: 0.2206, Time: 0.82s\n",
      "Batch 752/1946, Loss: 0.2500, Time: 0.82s\n",
      "Batch 753/1946, Loss: 0.2185, Time: 0.82s\n",
      "Batch 754/1946, Loss: 0.2602, Time: 0.82s\n",
      "Batch 755/1946, Loss: 0.2101, Time: 0.82s\n",
      "Batch 756/1946, Loss: 0.1762, Time: 0.82s\n",
      "Batch 757/1946, Loss: 0.2270, Time: 0.82s\n",
      "Batch 758/1946, Loss: 0.2077, Time: 0.82s\n",
      "Batch 759/1946, Loss: 0.2529, Time: 0.82s\n",
      "Batch 760/1946, Loss: 0.2101, Time: 0.82s\n",
      "Batch 761/1946, Loss: 0.2251, Time: 0.82s\n",
      "Batch 762/1946, Loss: 0.1964, Time: 0.82s\n",
      "Batch 763/1946, Loss: 0.2517, Time: 0.82s\n",
      "Batch 764/1946, Loss: 0.2086, Time: 0.82s\n",
      "Batch 765/1946, Loss: 0.2236, Time: 0.82s\n",
      "Batch 766/1946, Loss: 0.1602, Time: 0.83s\n",
      "Batch 767/1946, Loss: 0.1996, Time: 0.82s\n",
      "Batch 768/1946, Loss: 0.2343, Time: 0.82s\n",
      "Batch 769/1946, Loss: 0.2352, Time: 0.82s\n",
      "Batch 770/1946, Loss: 0.1685, Time: 0.82s\n",
      "Batch 771/1946, Loss: 0.2258, Time: 0.82s\n",
      "Batch 772/1946, Loss: 0.2656, Time: 0.82s\n",
      "Batch 773/1946, Loss: 0.2022, Time: 0.83s\n",
      "Batch 774/1946, Loss: 0.2219, Time: 0.82s\n",
      "Batch 775/1946, Loss: 0.1955, Time: 0.83s\n",
      "Batch 776/1946, Loss: 0.2601, Time: 0.82s\n",
      "Batch 777/1946, Loss: 0.1895, Time: 0.82s\n",
      "Batch 778/1946, Loss: 0.2106, Time: 0.82s\n",
      "Batch 779/1946, Loss: 0.2192, Time: 0.82s\n",
      "Batch 780/1946, Loss: 0.2220, Time: 0.82s\n",
      "Batch 781/1946, Loss: 0.1791, Time: 0.82s\n",
      "Batch 782/1946, Loss: 0.2268, Time: 0.82s\n",
      "Batch 783/1946, Loss: 0.1456, Time: 0.82s\n",
      "Batch 784/1946, Loss: 0.2052, Time: 0.82s\n",
      "Batch 785/1946, Loss: 0.2219, Time: 0.82s\n",
      "Batch 786/1946, Loss: 0.2274, Time: 0.82s\n",
      "Batch 787/1946, Loss: 0.1920, Time: 0.82s\n",
      "Batch 788/1946, Loss: 0.1893, Time: 0.83s\n",
      "Batch 789/1946, Loss: 0.2327, Time: 0.83s\n",
      "Batch 790/1946, Loss: 0.2311, Time: 0.83s\n",
      "Batch 791/1946, Loss: 0.2694, Time: 0.82s\n",
      "Batch 792/1946, Loss: 0.2206, Time: 0.82s\n",
      "Batch 793/1946, Loss: 0.2140, Time: 0.82s\n",
      "Batch 794/1946, Loss: 0.2252, Time: 0.82s\n",
      "Batch 795/1946, Loss: 0.2137, Time: 0.82s\n",
      "Batch 796/1946, Loss: 0.2516, Time: 0.82s\n",
      "Batch 797/1946, Loss: 0.1627, Time: 0.82s\n",
      "Batch 798/1946, Loss: 0.2157, Time: 0.83s\n",
      "Batch 799/1946, Loss: 0.2075, Time: 0.82s\n",
      "Batch 800/1946, Loss: 0.2775, Time: 0.82s\n",
      "Batch 801/1946, Loss: 0.2126, Time: 0.82s\n",
      "Batch 802/1946, Loss: 0.2273, Time: 0.82s\n",
      "Batch 803/1946, Loss: 0.2096, Time: 0.82s\n",
      "Batch 804/1946, Loss: 0.2172, Time: 0.82s\n",
      "Batch 805/1946, Loss: 0.1930, Time: 0.82s\n",
      "Batch 806/1946, Loss: 0.2212, Time: 0.82s\n",
      "Batch 807/1946, Loss: 0.2159, Time: 0.82s\n",
      "Batch 808/1946, Loss: 0.2077, Time: 0.82s\n",
      "Batch 809/1946, Loss: 0.2237, Time: 0.82s\n",
      "Batch 810/1946, Loss: 0.2344, Time: 0.82s\n",
      "Batch 811/1946, Loss: 0.2229, Time: 0.82s\n",
      "Batch 812/1946, Loss: 0.2536, Time: 0.82s\n",
      "Batch 813/1946, Loss: 0.2141, Time: 0.82s\n",
      "Batch 814/1946, Loss: 0.1702, Time: 0.83s\n",
      "Batch 815/1946, Loss: 0.2221, Time: 0.82s\n",
      "Batch 816/1946, Loss: 0.2128, Time: 0.82s\n",
      "Batch 817/1946, Loss: 0.1788, Time: 0.82s\n",
      "Batch 818/1946, Loss: 0.2274, Time: 0.83s\n",
      "Batch 819/1946, Loss: 0.2566, Time: 0.83s\n",
      "Batch 820/1946, Loss: 0.2306, Time: 0.83s\n",
      "Batch 821/1946, Loss: 0.2345, Time: 0.82s\n",
      "Batch 822/1946, Loss: 0.2192, Time: 0.83s\n",
      "Batch 823/1946, Loss: 0.2394, Time: 0.82s\n",
      "Batch 824/1946, Loss: 0.2608, Time: 0.83s\n",
      "Batch 825/1946, Loss: 0.2211, Time: 0.83s\n",
      "Batch 826/1946, Loss: 0.1919, Time: 0.83s\n",
      "Batch 827/1946, Loss: 0.2459, Time: 0.82s\n",
      "Batch 828/1946, Loss: 0.2045, Time: 0.82s\n",
      "Batch 829/1946, Loss: 0.1852, Time: 0.82s\n",
      "Batch 830/1946, Loss: 0.2942, Time: 0.82s\n",
      "Batch 831/1946, Loss: 0.2089, Time: 0.82s\n",
      "Batch 832/1946, Loss: 0.1992, Time: 0.82s\n",
      "Batch 833/1946, Loss: 0.2447, Time: 0.83s\n",
      "Batch 834/1946, Loss: 0.1861, Time: 0.82s\n",
      "Batch 835/1946, Loss: 0.2275, Time: 0.82s\n",
      "Batch 836/1946, Loss: 0.2382, Time: 0.82s\n",
      "Batch 837/1946, Loss: 0.2491, Time: 0.82s\n",
      "Batch 838/1946, Loss: 0.2154, Time: 0.82s\n",
      "Batch 839/1946, Loss: 0.2452, Time: 0.82s\n",
      "Batch 840/1946, Loss: 0.2059, Time: 0.82s\n",
      "Batch 841/1946, Loss: 0.1674, Time: 0.82s\n",
      "Batch 842/1946, Loss: 0.2086, Time: 0.82s\n",
      "Batch 843/1946, Loss: 0.2051, Time: 0.82s\n",
      "Batch 844/1946, Loss: 0.1835, Time: 0.82s\n",
      "Batch 845/1946, Loss: 0.2094, Time: 0.82s\n",
      "Batch 846/1946, Loss: 0.2227, Time: 0.82s\n",
      "Batch 847/1946, Loss: 0.2355, Time: 0.82s\n",
      "Batch 848/1946, Loss: 0.2107, Time: 0.82s\n",
      "Batch 849/1946, Loss: 0.2732, Time: 0.82s\n",
      "Batch 850/1946, Loss: 0.1874, Time: 0.82s\n",
      "Batch 851/1946, Loss: 0.1913, Time: 0.82s\n",
      "Batch 852/1946, Loss: 0.1710, Time: 0.82s\n",
      "Batch 853/1946, Loss: 0.2164, Time: 0.82s\n",
      "Batch 854/1946, Loss: 0.2600, Time: 0.82s\n",
      "Batch 855/1946, Loss: 0.1907, Time: 0.83s\n",
      "Batch 856/1946, Loss: 0.2004, Time: 0.83s\n",
      "Batch 857/1946, Loss: 0.2014, Time: 0.82s\n",
      "Batch 858/1946, Loss: 0.2089, Time: 0.82s\n",
      "Batch 859/1946, Loss: 0.2159, Time: 0.82s\n",
      "Batch 860/1946, Loss: 0.2022, Time: 0.82s\n",
      "Batch 861/1946, Loss: 0.1913, Time: 0.82s\n",
      "Batch 862/1946, Loss: 0.2371, Time: 0.82s\n",
      "Batch 863/1946, Loss: 0.2649, Time: 0.82s\n",
      "Batch 864/1946, Loss: 0.2453, Time: 0.82s\n",
      "Batch 865/1946, Loss: 0.2138, Time: 0.82s\n",
      "Batch 866/1946, Loss: 0.2685, Time: 0.82s\n",
      "Batch 867/1946, Loss: 0.1877, Time: 0.82s\n",
      "Batch 868/1946, Loss: 0.1801, Time: 0.83s\n",
      "Batch 869/1946, Loss: 0.2378, Time: 0.82s\n",
      "Batch 870/1946, Loss: 0.2289, Time: 0.82s\n",
      "Batch 871/1946, Loss: 0.2152, Time: 0.82s\n",
      "Batch 872/1946, Loss: 0.1952, Time: 0.82s\n",
      "Batch 873/1946, Loss: 0.2283, Time: 0.82s\n",
      "Batch 874/1946, Loss: 0.1821, Time: 0.82s\n",
      "Batch 875/1946, Loss: 0.1935, Time: 0.82s\n",
      "Batch 876/1946, Loss: 0.1770, Time: 0.82s\n",
      "Batch 877/1946, Loss: 0.2455, Time: 0.83s\n",
      "Batch 878/1946, Loss: 0.2298, Time: 0.83s\n",
      "Batch 879/1946, Loss: 0.1851, Time: 0.82s\n",
      "Batch 880/1946, Loss: 0.2208, Time: 0.82s\n",
      "Batch 881/1946, Loss: 0.2028, Time: 0.83s\n",
      "Batch 882/1946, Loss: 0.2227, Time: 0.82s\n",
      "Batch 883/1946, Loss: 0.2062, Time: 0.82s\n",
      "Batch 884/1946, Loss: 0.2069, Time: 0.82s\n",
      "Batch 885/1946, Loss: 0.1997, Time: 0.82s\n",
      "Batch 886/1946, Loss: 0.1622, Time: 0.82s\n",
      "Batch 887/1946, Loss: 0.1978, Time: 0.82s\n",
      "Batch 888/1946, Loss: 0.2064, Time: 0.83s\n",
      "Batch 889/1946, Loss: 0.1930, Time: 0.83s\n",
      "Batch 890/1946, Loss: 0.2725, Time: 0.82s\n",
      "Batch 891/1946, Loss: 0.2511, Time: 0.82s\n",
      "Batch 892/1946, Loss: 0.2203, Time: 0.82s\n",
      "Batch 893/1946, Loss: 0.1927, Time: 0.82s\n",
      "Batch 894/1946, Loss: 0.2236, Time: 0.82s\n",
      "Batch 895/1946, Loss: 0.2157, Time: 0.82s\n",
      "Batch 896/1946, Loss: 0.2167, Time: 0.82s\n",
      "Batch 897/1946, Loss: 0.2311, Time: 0.82s\n",
      "Batch 898/1946, Loss: 0.2225, Time: 0.82s\n",
      "Batch 899/1946, Loss: 0.2002, Time: 0.82s\n",
      "Batch 900/1946, Loss: 0.1827, Time: 0.83s\n",
      "Batch 901/1946, Loss: 0.2219, Time: 0.82s\n",
      "Batch 902/1946, Loss: 0.2164, Time: 0.82s\n",
      "Batch 903/1946, Loss: 0.2565, Time: 0.82s\n",
      "Batch 904/1946, Loss: 0.2147, Time: 0.83s\n",
      "Batch 905/1946, Loss: 0.2671, Time: 0.83s\n",
      "Batch 906/1946, Loss: 0.2183, Time: 0.82s\n",
      "Batch 907/1946, Loss: 0.1828, Time: 0.82s\n",
      "Batch 908/1946, Loss: 0.1750, Time: 0.83s\n",
      "Batch 909/1946, Loss: 0.1962, Time: 0.83s\n",
      "Batch 910/1946, Loss: 0.2192, Time: 0.83s\n",
      "Batch 911/1946, Loss: 0.2360, Time: 0.82s\n",
      "Batch 912/1946, Loss: 0.2321, Time: 0.83s\n",
      "Batch 913/1946, Loss: 0.1945, Time: 0.83s\n",
      "Batch 914/1946, Loss: 0.2280, Time: 0.83s\n",
      "Batch 915/1946, Loss: 0.2641, Time: 0.83s\n",
      "Batch 916/1946, Loss: 0.2403, Time: 0.82s\n",
      "Batch 917/1946, Loss: 0.2379, Time: 0.82s\n",
      "Batch 918/1946, Loss: 0.2481, Time: 0.82s\n",
      "Batch 919/1946, Loss: 0.1974, Time: 0.82s\n",
      "Batch 920/1946, Loss: 0.1771, Time: 0.83s\n",
      "Batch 921/1946, Loss: 0.2251, Time: 0.83s\n",
      "Batch 922/1946, Loss: 0.2691, Time: 0.83s\n",
      "Batch 923/1946, Loss: 0.2506, Time: 0.82s\n",
      "Batch 924/1946, Loss: 0.2145, Time: 0.83s\n",
      "Batch 925/1946, Loss: 0.2014, Time: 0.83s\n",
      "Batch 926/1946, Loss: 0.2167, Time: 0.82s\n",
      "Batch 927/1946, Loss: 0.2527, Time: 0.82s\n",
      "Batch 928/1946, Loss: 0.2513, Time: 0.83s\n",
      "Batch 929/1946, Loss: 0.1818, Time: 0.83s\n",
      "Batch 930/1946, Loss: 0.2348, Time: 0.82s\n",
      "Batch 931/1946, Loss: 0.2342, Time: 0.82s\n",
      "Batch 932/1946, Loss: 0.2526, Time: 0.82s\n",
      "Batch 933/1946, Loss: 0.2240, Time: 0.82s\n",
      "Batch 934/1946, Loss: 0.2310, Time: 0.82s\n",
      "Batch 935/1946, Loss: 0.2221, Time: 0.82s\n",
      "Batch 936/1946, Loss: 0.2114, Time: 0.82s\n",
      "Batch 937/1946, Loss: 0.1833, Time: 0.82s\n",
      "Batch 938/1946, Loss: 0.2281, Time: 0.82s\n",
      "Batch 939/1946, Loss: 0.2091, Time: 0.82s\n",
      "Batch 940/1946, Loss: 0.1955, Time: 0.82s\n",
      "Batch 941/1946, Loss: 0.2382, Time: 0.82s\n",
      "Batch 942/1946, Loss: 0.2177, Time: 0.83s\n",
      "Batch 943/1946, Loss: 0.2001, Time: 0.83s\n",
      "Batch 944/1946, Loss: 0.2808, Time: 0.82s\n",
      "Batch 945/1946, Loss: 0.2337, Time: 0.83s\n",
      "Batch 946/1946, Loss: 0.2082, Time: 0.83s\n",
      "Batch 947/1946, Loss: 0.2354, Time: 0.83s\n",
      "Batch 948/1946, Loss: 0.1903, Time: 0.82s\n",
      "Batch 949/1946, Loss: 0.2362, Time: 0.83s\n",
      "Batch 950/1946, Loss: 0.2017, Time: 0.82s\n",
      "Batch 951/1946, Loss: 0.1903, Time: 0.82s\n",
      "Batch 952/1946, Loss: 0.2473, Time: 0.82s\n",
      "Batch 953/1946, Loss: 0.2340, Time: 0.82s\n",
      "Batch 954/1946, Loss: 0.2268, Time: 0.82s\n",
      "Batch 955/1946, Loss: 0.2387, Time: 0.82s\n",
      "Batch 956/1946, Loss: 0.2131, Time: 0.82s\n",
      "Batch 957/1946, Loss: 0.1716, Time: 0.83s\n",
      "Batch 958/1946, Loss: 0.2603, Time: 0.83s\n",
      "Batch 959/1946, Loss: 0.1763, Time: 0.83s\n",
      "Batch 960/1946, Loss: 0.2373, Time: 0.83s\n",
      "Batch 961/1946, Loss: 0.2525, Time: 0.82s\n",
      "Batch 962/1946, Loss: 0.1949, Time: 0.82s\n",
      "Batch 963/1946, Loss: 0.2682, Time: 0.82s\n",
      "Batch 964/1946, Loss: 0.1971, Time: 0.82s\n",
      "Batch 965/1946, Loss: 0.2357, Time: 0.82s\n",
      "Batch 966/1946, Loss: 0.2189, Time: 0.82s\n",
      "Batch 967/1946, Loss: 0.2049, Time: 0.82s\n",
      "Batch 968/1946, Loss: 0.2723, Time: 0.82s\n",
      "Batch 969/1946, Loss: 0.2636, Time: 0.82s\n",
      "Batch 970/1946, Loss: 0.1912, Time: 0.82s\n",
      "Batch 971/1946, Loss: 0.2190, Time: 0.83s\n",
      "Batch 972/1946, Loss: 0.2633, Time: 0.82s\n",
      "Batch 973/1946, Loss: 0.2404, Time: 0.83s\n",
      "Batch 974/1946, Loss: 0.2349, Time: 0.83s\n",
      "Batch 975/1946, Loss: 0.2414, Time: 0.83s\n",
      "Batch 976/1946, Loss: 0.2245, Time: 0.83s\n",
      "Batch 977/1946, Loss: 0.2616, Time: 0.82s\n",
      "Batch 978/1946, Loss: 0.2538, Time: 0.82s\n",
      "Batch 979/1946, Loss: 0.2002, Time: 0.82s\n",
      "Batch 980/1946, Loss: 0.2278, Time: 0.82s\n",
      "Batch 981/1946, Loss: 0.2018, Time: 0.83s\n",
      "Batch 982/1946, Loss: 0.1821, Time: 0.83s\n",
      "Batch 983/1946, Loss: 0.2230, Time: 0.83s\n",
      "Batch 984/1946, Loss: 0.2054, Time: 0.82s\n",
      "Batch 985/1946, Loss: 0.2210, Time: 0.82s\n",
      "Batch 986/1946, Loss: 0.2287, Time: 0.82s\n",
      "Batch 987/1946, Loss: 0.2082, Time: 0.82s\n",
      "Batch 988/1946, Loss: 0.2289, Time: 0.82s\n",
      "Batch 989/1946, Loss: 0.1947, Time: 0.83s\n",
      "Batch 990/1946, Loss: 0.2357, Time: 0.82s\n",
      "Batch 991/1946, Loss: 0.2313, Time: 0.83s\n",
      "Batch 992/1946, Loss: 0.2032, Time: 0.82s\n",
      "Batch 993/1946, Loss: 0.2435, Time: 0.82s\n",
      "Batch 994/1946, Loss: 0.2375, Time: 0.82s\n",
      "Batch 995/1946, Loss: 0.2468, Time: 0.82s\n",
      "Batch 996/1946, Loss: 0.2403, Time: 0.82s\n",
      "Batch 997/1946, Loss: 0.2497, Time: 0.83s\n",
      "Batch 998/1946, Loss: 0.1965, Time: 0.82s\n",
      "Batch 999/1946, Loss: 0.2225, Time: 0.82s\n",
      "Batch 1000/1946, Loss: 0.1854, Time: 0.82s\n",
      "Batch 1001/1946, Loss: 0.2665, Time: 0.83s\n",
      "Batch 1002/1946, Loss: 0.2156, Time: 0.83s\n",
      "Batch 1003/1946, Loss: 0.2342, Time: 0.82s\n",
      "Batch 1004/1946, Loss: 0.2465, Time: 0.82s\n",
      "Batch 1005/1946, Loss: 0.1875, Time: 0.82s\n",
      "Batch 1006/1946, Loss: 0.2310, Time: 0.82s\n",
      "Batch 1007/1946, Loss: 0.2199, Time: 0.82s\n",
      "Batch 1008/1946, Loss: 0.2264, Time: 0.82s\n",
      "Batch 1009/1946, Loss: 0.2607, Time: 0.83s\n",
      "Batch 1010/1946, Loss: 0.2036, Time: 0.83s\n",
      "Batch 1011/1946, Loss: 0.2200, Time: 0.83s\n",
      "Batch 1012/1946, Loss: 0.2533, Time: 0.83s\n",
      "Batch 1013/1946, Loss: 0.1903, Time: 0.83s\n",
      "Batch 1014/1946, Loss: 0.2402, Time: 0.83s\n",
      "Batch 1015/1946, Loss: 0.2253, Time: 0.83s\n",
      "Batch 1016/1946, Loss: 0.2122, Time: 0.83s\n",
      "Batch 1017/1946, Loss: 0.2070, Time: 0.83s\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += losses.item()\n",
    "        \n",
    "        batch_end_time = time.time()\n",
    "        print(f\"Batch {batch_idx+1}/{num_batches}, Loss: {losses.item():.4f}, Time: {batch_end_time - batch_start_time:.2f}s\")\n",
    "    \n",
    "    train_loss /= num_batches\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} training completed. Average Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    num_val_batches = len(val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(val_loader):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            loss_dict = model(images, targets)\n",
    "            print(f\"Validation loss_dict: {loss_dict}\")  # Debugging print statement\n",
    "            if isinstance(loss_dict, dict):\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "            elif isinstance(loss_dict, list):\n",
    "                # If it's a list of dicts, sum the values within each dict\n",
    "                losses = sum(sum(loss for loss in d.values()) for d in loss_dict)\n",
    "            else:\n",
    "                raise TypeError(\"Unexpected type for loss_dict\")\n",
    "            \n",
    "            val_loss += losses.item()\n",
    "    \n",
    "    val_loss /= num_val_batches\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} validation completed. Average Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    epoch_end_time = time.time()\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_end_time - epoch_start_time:.2f}s\\n\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), '../model/fasterrcnn_coconut_tree_detector5.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
